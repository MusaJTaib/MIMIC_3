{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureSetB = pd.read_csv('MIMIC_Windows/FSB_5Ws.csv')\n",
    "FeatureMortality = pd.read_csv('MIMIC_Windows/FSB_Mortality.csv')\n",
    "numberOfWindows = 5\n",
    "numberOfFeatures = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Subject_id = pd.DataFrame()\n",
    "Subject_id = FeatureSetB.subject_id\n",
    "Subject_id.drop_duplicates(keep = 'first', inplace = True)\n",
    "Subject_id.reset_index(drop=True,inplace=True)\n",
    "NumSubjects = len(Subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = FeatureMortality['subject_id'].equals(Subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureSet = FeatureSetB.drop(['subject_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureSet = FeatureSet.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = pd.DataFrame()\n",
    "Labels['Expired'] = FeatureMortality['Expired'] \n",
    "y_values = Labels.to_numpy()\n",
    "y_values = y_values.flatten()\n",
    "nClients = NumSubjects\n",
    "X = FeatureSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "\n",
    "acc_score = []\n",
    "re_score = []\n",
    "pre_score = []\n",
    "f_score = []\n",
    "auroc = []\n",
    "auprc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(123)\n",
    "\n",
    "import sys\n",
    "mod=sys.modules[__name__]\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model =  LogisticRegression(max_iter = 1000)\n",
    "from sklearn import metrics\n",
    "\n",
    "acc_score = []\n",
    "re_score = []\n",
    "pre_score = []\n",
    "f_score = []\n",
    "auroc = []\n",
    "auprc = []\n",
    "\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "\n",
    "y = y_values\n",
    "resample = NeighbourhoodCleaningRule()\n",
    "k = 5\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "Train_shape = X.shape\n",
    "X = X.reshape(int(Train_shape[0]/numberOfWindows),numberOfWindows*numberOfFeatures)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "for train_ix, val_ix in skf.split(X,y):\n",
    "    # select rows\n",
    "    X_train, X_val = X[train_ix], X[val_ix]\n",
    "    y_train, y_val = y[train_ix], y[val_ix]\n",
    "\n",
    "    X_train,y_train = resample.fit_resample(X_train, y_train) \n",
    "\n",
    "    history4 = model.fit(X_train,y_train) \n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred = list(map(lambda x: 0 if x<=0.5 else 1, y_pred))\n",
    "\n",
    "    y_actu = pd.Series(y_val)\n",
    "    y_pred = pd.Series(y_pred)\n",
    "    \n",
    "    f1_value = f1_score(y_actu, y_pred,pos_label = 1, average='binary') \n",
    "    sensitivity = recall_score(y_actu, y_pred,pos_label = 1, average='binary')\n",
    "    precision = precision_score(y_actu, y_pred,pos_label = 1, average='binary')\n",
    "    prc = average_precision_score(y_actu, y_pred,pos_label = 1)\n",
    "    roc = roc_auc_score(y_actu, y_pred)\n",
    "    accuracy = accuracy_score(y_actu, y_pred)\n",
    "    \n",
    "    acc_score.append(accuracy)\n",
    "    re_score.append(sensitivity)\n",
    "    pre_score.append(precision)\n",
    "    f_score.append(f1_value)\n",
    "    auroc.append(roc)\n",
    "    auprc.append(prc)\n",
    "   \n",
    "avg_acc_score = sum(acc_score)/k\n",
    "avg_recall_score = sum(re_score)/k\n",
    "avg_precision_score = sum(pre_score)/k\n",
    "avg_f1_score = sum(f_score)/k\n",
    "avg_roc_score = sum(auroc)/k\n",
    "avg_prc_score = sum(auprc)/k\n",
    "\n",
    "sensitivity = avg_recall_score\n",
    "precision = avg_precision_score\n",
    "accuracy = avg_acc_score\n",
    "f1_score = avg_f1_score\n",
    "auroc = avg_roc_score\n",
    "auprc = avg_prc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of each fold - [0.9090909090909091, 0.9152097902097902, 0.9055944055944056, 0.9055944055944056, 0.9011373578302713]\n",
      "Avg accuracy : 0.9073253736639565\n",
      "Recall of each fold - [0.3883495145631068, 0.4368932038834951, 0.3942307692307692, 0.4423076923076923, 0.39805825242718446]\n",
      "Avg Reccall : 0.4119678864824496\n",
      "Precision of each fold - [0.49382716049382713, 0.5357142857142857, 0.47674418604651164, 0.4791666666666667, 0.44565217391304346]\n",
      "Avg Precision : 0.48622089456686696\n",
      "F1_Score of each fold - [0.43478260869565216, 0.48128342245989303, 0.43157894736842106, 0.45999999999999996, 0.42051282051282046]\n",
      "Avg F1_score : 0.4456315598073573\n",
      "AUROC of each fold - 0.6842605633550207\n",
      "Avg AUROC : 0.6842605633550207\n",
      "AUPRC of each fold - 0.2537782010274914\n",
      "Avg AUPRC : 0.2537782010274914\n"
     ]
    }
   ],
   "source": [
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))\n",
    "print('Recall of each fold - {}'.format(re_score))\n",
    "print('Avg Reccall : {}'.format(avg_recall_score))\n",
    "print('Precision of each fold - {}'.format(pre_score))\n",
    "print('Avg Precision : {}'.format(avg_precision_score))\n",
    "print('F1_Score of each fold - {}'.format(f_score))\n",
    "print('Avg F1_score : {}'.format(avg_f1_score)) \n",
    "print('AUROC of each fold - {}'.format(auroc))\n",
    "print('Avg AUROC : {}'.format(avg_roc_score))\n",
    "print('AUPRC of each fold - {}'.format(auprc))\n",
    "print('Avg AUPRC : {}'.format(avg_prc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "\n",
    "csv_columns = ['model-type','precision','sensitivity','f1-score','accuracy','AUROC','AUPRC','NumberOfWindows']\n",
    "dict_data = [{'model-type':'LR', 'precision': precision,'sensitivity': sensitivity,'f1-score': f1_score,'accuracy': accuracy,'NumberOfWindows':numberOfWindows,'AUROC':auroc,'AUPRC':auprc}]\n",
    "metric_file = \"Results/Results_LR.csv\"\n",
    "\n",
    "file_exists = os.path.isfile(metric_file)\n",
    "try:\n",
    "    with open(metric_file, 'a') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        for data in dict_data:\n",
    "            writer.writerow(data)\n",
    "except IOError:\n",
    "    print(\"I/O error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48feb6360a00eb0f47f9c89bd4d436ba37b01dbe1c2db3c2c6dfcc0146625bee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
