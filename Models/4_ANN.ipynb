{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureSetB = pd.read_csv('MIMIC_Windows/FSB_1Ws.csv')\n",
    "FeatureMortality = pd.read_csv('MIMIC_Windows/FSB_Mortality.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfWindows = 1\n",
    "numberOfFeatures = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Subject_id = pd.DataFrame()\n",
    "Subject_id = FeatureSetB.subject_id\n",
    "Subject_id.drop_duplicates(keep = 'first', inplace = True)\n",
    "Subject_id.reset_index(drop=True,inplace=True)\n",
    "NumSubjects = len(Subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = FeatureMortality['subject_id'].equals(Subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureSet = FeatureSetB.drop(['subject_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureSet = FeatureSet.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = pd.DataFrame()\n",
    "Labels['Expired'] = FeatureMortality['Expired'] \n",
    "y_values = Labels.to_numpy()\n",
    "y_values = y_values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nClients = NumSubjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from sklearn import metrics\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "mod=sys.modules[__name__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR = [\n",
    "      keras.metrics.AUC(name='prc',multi_label=True,curve='PR'),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               2100      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,301\n",
      "Trainable params: 12,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "tf.random.set_seed(1234)  # Fixing the seed values for reproduciblity\n",
    "\n",
    "model = Sequential()  \n",
    "model.add(Flatten(input_shape=(numberOfWindows,numberOfFeatures)))\n",
    "model.add(Dense(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()  #number of parameters in dense layer is Wx + b, output=# neurons, (param = output * input + output * b)\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=PR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-02 08:55:52.792390: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 2s 15ms/step - loss: 0.3455 - prc: 0.2458\n",
      "Epoch 2/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.2333 - prc: 0.4560\n",
      "Epoch 3/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.2214 - prc: 0.4785\n",
      "Epoch 4/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.2126 - prc: 0.5201\n",
      "Epoch 5/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.2070 - prc: 0.5398\n",
      "Epoch 6/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.2027 - prc: 0.5462\n",
      "Epoch 7/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1994 - prc: 0.5562\n",
      "Epoch 8/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1952 - prc: 0.5780\n",
      "Epoch 9/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1952 - prc: 0.5714\n",
      "Epoch 10/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1930 - prc: 0.5841\n",
      "Epoch 11/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1915 - prc: 0.5842\n",
      "Epoch 12/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1869 - prc: 0.5995\n",
      "Epoch 13/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1906 - prc: 0.5827\n",
      "Epoch 14/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1838 - prc: 0.6106\n",
      "Epoch 15/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1847 - prc: 0.6093\n",
      "Epoch 16/75\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.1838 - prc: 0.6099\n",
      "Epoch 17/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1836 - prc: 0.6044\n",
      "Epoch 18/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1813 - prc: 0.6178\n",
      "Epoch 19/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1789 - prc: 0.6277\n",
      "Epoch 20/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1784 - prc: 0.6312\n",
      "Epoch 21/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1741 - prc: 0.6539\n",
      "Epoch 22/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1766 - prc: 0.6296\n",
      "Epoch 23/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1758 - prc: 0.6381\n",
      "Epoch 24/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1728 - prc: 0.6512\n",
      "Epoch 25/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1712 - prc: 0.6572\n",
      "Epoch 26/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1687 - prc: 0.6715\n",
      "Epoch 27/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1710 - prc: 0.6539\n",
      "Epoch 28/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1757 - prc: 0.6319\n",
      "Epoch 29/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1685 - prc: 0.6646\n",
      "Epoch 30/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1698 - prc: 0.6598\n",
      "Epoch 31/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1657 - prc: 0.6743\n",
      "Epoch 32/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1675 - prc: 0.6682\n",
      "Epoch 33/75\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.1657 - prc: 0.6675\n",
      "Epoch 34/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1650 - prc: 0.6777\n",
      "Epoch 35/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1635 - prc: 0.6832\n",
      "Epoch 36/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1636 - prc: 0.6703\n",
      "Epoch 37/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1622 - prc: 0.6847\n",
      "Epoch 38/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1599 - prc: 0.6939\n",
      "Epoch 39/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1602 - prc: 0.6908\n",
      "Epoch 40/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1610 - prc: 0.6829\n",
      "Epoch 41/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1588 - prc: 0.6885\n",
      "Epoch 42/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1596 - prc: 0.6896\n",
      "Epoch 43/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1570 - prc: 0.7022\n",
      "Epoch 44/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1562 - prc: 0.7022\n",
      "Epoch 45/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1541 - prc: 0.7157\n",
      "Epoch 46/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1524 - prc: 0.7207\n",
      "Epoch 47/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1553 - prc: 0.7034\n",
      "Epoch 48/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1513 - prc: 0.7244\n",
      "Epoch 49/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1571 - prc: 0.7006\n",
      "Epoch 50/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1523 - prc: 0.7193\n",
      "Epoch 51/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1528 - prc: 0.7173\n",
      "Epoch 52/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1489 - prc: 0.7307\n",
      "Epoch 53/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1542 - prc: 0.7081\n",
      "Epoch 54/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1501 - prc: 0.7196\n",
      "Epoch 55/75\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.1488 - prc: 0.7244\n",
      "Epoch 56/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1453 - prc: 0.7415\n",
      "Epoch 57/75\n",
      "72/72 [==============================] - 1s 16ms/step - loss: 0.1484 - prc: 0.7277\n",
      "Epoch 58/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1476 - prc: 0.7312\n",
      "Epoch 59/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1445 - prc: 0.7436\n",
      "Epoch 60/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1448 - prc: 0.7474\n",
      "Epoch 61/75\n",
      "72/72 [==============================] - 1s 15ms/step - loss: 0.1477 - prc: 0.7308\n",
      "Epoch 62/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1443 - prc: 0.7474\n",
      "Epoch 63/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1404 - prc: 0.7530\n",
      "Epoch 64/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1375 - prc: 0.7653\n",
      "Epoch 65/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1436 - prc: 0.7443\n",
      "Epoch 66/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1425 - prc: 0.7497\n",
      "Epoch 67/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1399 - prc: 0.7631\n",
      "Epoch 68/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1393 - prc: 0.7640\n",
      "Epoch 69/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1365 - prc: 0.7682\n",
      "Epoch 70/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1356 - prc: 0.7724\n",
      "Epoch 71/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1385 - prc: 0.7620\n",
      "Epoch 72/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1390 - prc: 0.7561\n",
      "Epoch 73/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1347 - prc: 0.7749\n",
      "Epoch 74/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1339 - prc: 0.7756\n",
      "Epoch 75/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1351 - prc: 0.7667\n",
      "36/36 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-02 08:57:00.525009: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1654 - prc: 0.6673\n",
      "Epoch 2/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1572 - prc: 0.6917\n",
      "Epoch 3/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1575 - prc: 0.6931\n",
      "Epoch 4/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1522 - prc: 0.7141\n",
      "Epoch 5/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1499 - prc: 0.7148\n",
      "Epoch 6/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1505 - prc: 0.7112\n",
      "Epoch 7/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1459 - prc: 0.7344\n",
      "Epoch 8/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1463 - prc: 0.7307\n",
      "Epoch 9/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1466 - prc: 0.7240\n",
      "Epoch 10/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1455 - prc: 0.7357\n",
      "Epoch 11/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1458 - prc: 0.7351\n",
      "Epoch 12/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1429 - prc: 0.7386\n",
      "Epoch 13/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1388 - prc: 0.7600\n",
      "Epoch 14/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1395 - prc: 0.7540\n",
      "Epoch 15/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1348 - prc: 0.7618\n",
      "Epoch 16/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1370 - prc: 0.7554\n",
      "Epoch 17/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1345 - prc: 0.7719\n",
      "Epoch 18/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1316 - prc: 0.7800\n",
      "Epoch 19/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1328 - prc: 0.7699\n",
      "Epoch 20/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1346 - prc: 0.7660\n",
      "Epoch 21/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1323 - prc: 0.7743\n",
      "Epoch 22/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1296 - prc: 0.7841\n",
      "Epoch 23/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1291 - prc: 0.7909\n",
      "Epoch 24/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1275 - prc: 0.7898\n",
      "Epoch 25/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1270 - prc: 0.7925\n",
      "Epoch 26/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1251 - prc: 0.8025\n",
      "Epoch 27/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1255 - prc: 0.8004\n",
      "Epoch 28/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1277 - prc: 0.7846\n",
      "Epoch 29/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1257 - prc: 0.7999\n",
      "Epoch 30/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1235 - prc: 0.7974\n",
      "Epoch 31/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1256 - prc: 0.7977\n",
      "Epoch 32/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1235 - prc: 0.8016\n",
      "Epoch 33/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1255 - prc: 0.7941\n",
      "Epoch 34/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1229 - prc: 0.8076\n",
      "Epoch 35/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1233 - prc: 0.8003\n",
      "Epoch 36/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1185 - prc: 0.8213\n",
      "Epoch 37/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1194 - prc: 0.8130\n",
      "Epoch 38/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1206 - prc: 0.8127\n",
      "Epoch 39/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1155 - prc: 0.8283\n",
      "Epoch 40/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1201 - prc: 0.8115\n",
      "Epoch 41/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1190 - prc: 0.8143\n",
      "Epoch 42/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1111 - prc: 0.8451\n",
      "Epoch 43/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1154 - prc: 0.8228\n",
      "Epoch 44/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1109 - prc: 0.8384\n",
      "Epoch 45/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1151 - prc: 0.8296\n",
      "Epoch 46/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1162 - prc: 0.8270\n",
      "Epoch 47/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1135 - prc: 0.8319\n",
      "Epoch 48/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1125 - prc: 0.8361\n",
      "Epoch 49/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1092 - prc: 0.8473\n",
      "Epoch 50/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1100 - prc: 0.8405\n",
      "Epoch 51/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1101 - prc: 0.8415\n",
      "Epoch 52/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1152 - prc: 0.8220\n",
      "Epoch 53/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1052 - prc: 0.8521\n",
      "Epoch 54/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1080 - prc: 0.8457\n",
      "Epoch 55/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1098 - prc: 0.8415\n",
      "Epoch 56/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1044 - prc: 0.8587\n",
      "Epoch 57/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1053 - prc: 0.8525\n",
      "Epoch 58/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1061 - prc: 0.8529\n",
      "Epoch 59/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1079 - prc: 0.8407\n",
      "Epoch 60/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1057 - prc: 0.8522\n",
      "Epoch 61/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1004 - prc: 0.8664\n",
      "Epoch 62/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1001 - prc: 0.8721\n",
      "Epoch 63/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1010 - prc: 0.8670\n",
      "Epoch 64/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0989 - prc: 0.8709\n",
      "Epoch 65/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0984 - prc: 0.8747\n",
      "Epoch 66/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0946 - prc: 0.8842\n",
      "Epoch 67/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0981 - prc: 0.8731\n",
      "Epoch 68/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0989 - prc: 0.8708\n",
      "Epoch 69/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0973 - prc: 0.8719\n",
      "Epoch 70/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0965 - prc: 0.8740\n",
      "Epoch 71/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0985 - prc: 0.8697\n",
      "Epoch 72/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0985 - prc: 0.8743\n",
      "Epoch 73/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0968 - prc: 0.8709\n",
      "Epoch 74/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0920 - prc: 0.8922\n",
      "Epoch 75/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0943 - prc: 0.8800\n",
      "36/36 [==============================] - 0s 2ms/step\n",
      "Epoch 1/75\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.1353 - prc: 0.7724\n",
      "Epoch 2/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1289 - prc: 0.7937\n",
      "Epoch 3/75\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.1167 - prc: 0.8172\n",
      "Epoch 4/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1143 - prc: 0.8292\n",
      "Epoch 5/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1189 - prc: 0.8179\n",
      "Epoch 6/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1114 - prc: 0.8336\n",
      "Epoch 7/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1119 - prc: 0.8370\n",
      "Epoch 8/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1119 - prc: 0.8320\n",
      "Epoch 9/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1081 - prc: 0.8453\n",
      "Epoch 10/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1072 - prc: 0.8508\n",
      "Epoch 11/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1061 - prc: 0.8464\n",
      "Epoch 12/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1057 - prc: 0.8490\n",
      "Epoch 13/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1024 - prc: 0.8627\n",
      "Epoch 14/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1061 - prc: 0.8531\n",
      "Epoch 15/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1079 - prc: 0.8412\n",
      "Epoch 16/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1042 - prc: 0.8510\n",
      "Epoch 17/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1025 - prc: 0.8531\n",
      "Epoch 18/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0977 - prc: 0.8717\n",
      "Epoch 19/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1004 - prc: 0.8624\n",
      "Epoch 20/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1002 - prc: 0.8641\n",
      "Epoch 21/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0920 - prc: 0.8908\n",
      "Epoch 22/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0906 - prc: 0.8921\n",
      "Epoch 23/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0970 - prc: 0.8735\n",
      "Epoch 24/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0958 - prc: 0.8743\n",
      "Epoch 25/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0944 - prc: 0.8800\n",
      "Epoch 26/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0942 - prc: 0.8768\n",
      "Epoch 27/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0902 - prc: 0.8867\n",
      "Epoch 28/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0907 - prc: 0.8898\n",
      "Epoch 29/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0889 - prc: 0.8929\n",
      "Epoch 30/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0907 - prc: 0.8904\n",
      "Epoch 31/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0869 - prc: 0.8970\n",
      "Epoch 32/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0867 - prc: 0.8979\n",
      "Epoch 33/75\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.0850 - prc: 0.9039\n",
      "Epoch 34/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0907 - prc: 0.8859\n",
      "Epoch 35/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0868 - prc: 0.8972\n",
      "Epoch 36/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0864 - prc: 0.8982\n",
      "Epoch 37/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0849 - prc: 0.8996\n",
      "Epoch 38/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0930 - prc: 0.8769\n",
      "Epoch 39/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0862 - prc: 0.9002\n",
      "Epoch 40/75\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.0805 - prc: 0.9136\n",
      "Epoch 41/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0831 - prc: 0.9087\n",
      "Epoch 42/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0837 - prc: 0.9014\n",
      "Epoch 43/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0817 - prc: 0.9105\n",
      "Epoch 44/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0822 - prc: 0.9092\n",
      "Epoch 45/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0775 - prc: 0.9185\n",
      "Epoch 46/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0809 - prc: 0.9112\n",
      "Epoch 47/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0835 - prc: 0.8995\n",
      "Epoch 48/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0832 - prc: 0.9038\n",
      "Epoch 49/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0775 - prc: 0.9193\n",
      "Epoch 50/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0768 - prc: 0.9194\n",
      "Epoch 51/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0752 - prc: 0.9236\n",
      "Epoch 52/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0797 - prc: 0.9125\n",
      "Epoch 53/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0782 - prc: 0.9145\n",
      "Epoch 54/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0777 - prc: 0.9180\n",
      "Epoch 55/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0737 - prc: 0.9270\n",
      "Epoch 56/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0734 - prc: 0.9241\n",
      "Epoch 57/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0741 - prc: 0.9241\n",
      "Epoch 58/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0778 - prc: 0.9159\n",
      "Epoch 59/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0772 - prc: 0.9179\n",
      "Epoch 60/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0722 - prc: 0.9264\n",
      "Epoch 61/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0747 - prc: 0.9236\n",
      "Epoch 62/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0735 - prc: 0.9268\n",
      "Epoch 63/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0703 - prc: 0.9354\n",
      "Epoch 64/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0725 - prc: 0.9294\n",
      "Epoch 65/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0709 - prc: 0.9318\n",
      "Epoch 66/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0722 - prc: 0.9274\n",
      "Epoch 67/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0706 - prc: 0.9319\n",
      "Epoch 68/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0726 - prc: 0.9266\n",
      "Epoch 69/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0764 - prc: 0.9165\n",
      "Epoch 70/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0716 - prc: 0.9284\n",
      "Epoch 71/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0683 - prc: 0.9391\n",
      "Epoch 72/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0703 - prc: 0.9306\n",
      "Epoch 73/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0684 - prc: 0.9336\n",
      "Epoch 74/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0709 - prc: 0.9291\n",
      "Epoch 75/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0671 - prc: 0.9391\n",
      "36/36 [==============================] - 0s 3ms/step\n",
      "Epoch 1/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1150 - prc: 0.8293\n",
      "Epoch 2/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1070 - prc: 0.8488\n",
      "Epoch 3/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0997 - prc: 0.8636\n",
      "Epoch 4/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0990 - prc: 0.8786\n",
      "Epoch 5/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.1055 - prc: 0.8516\n",
      "Epoch 6/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.1074 - prc: 0.8500\n",
      "Epoch 7/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0981 - prc: 0.8697\n",
      "Epoch 8/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0830 - prc: 0.9018\n",
      "Epoch 9/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0974 - prc: 0.8804\n",
      "Epoch 10/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0827 - prc: 0.9045\n",
      "Epoch 11/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0851 - prc: 0.8951\n",
      "Epoch 12/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0847 - prc: 0.9010\n",
      "Epoch 13/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0747 - prc: 0.9228\n",
      "Epoch 14/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0789 - prc: 0.9130\n",
      "Epoch 15/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0828 - prc: 0.9021\n",
      "Epoch 16/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0836 - prc: 0.8984\n",
      "Epoch 17/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0786 - prc: 0.9099\n",
      "Epoch 18/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0706 - prc: 0.9321\n",
      "Epoch 19/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0701 - prc: 0.9334\n",
      "Epoch 20/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0725 - prc: 0.9261\n",
      "Epoch 21/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0762 - prc: 0.9149\n",
      "Epoch 22/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0721 - prc: 0.9258\n",
      "Epoch 23/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0709 - prc: 0.9269\n",
      "Epoch 24/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0763 - prc: 0.9152\n",
      "Epoch 25/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0781 - prc: 0.9250\n",
      "Epoch 26/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0892 - prc: 0.9080\n",
      "Epoch 27/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0806 - prc: 0.9104\n",
      "Epoch 28/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0690 - prc: 0.9314\n",
      "Epoch 29/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0653 - prc: 0.9416\n",
      "Epoch 30/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0736 - prc: 0.9220\n",
      "Epoch 31/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0795 - prc: 0.8979\n",
      "Epoch 32/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0697 - prc: 0.9286\n",
      "Epoch 33/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0713 - prc: 0.9262\n",
      "Epoch 34/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0657 - prc: 0.9390\n",
      "Epoch 35/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0660 - prc: 0.9378\n",
      "Epoch 36/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0627 - prc: 0.9456\n",
      "Epoch 37/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0623 - prc: 0.9455\n",
      "Epoch 38/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0666 - prc: 0.9351\n",
      "Epoch 39/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0639 - prc: 0.9411\n",
      "Epoch 40/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0715 - prc: 0.9347\n",
      "Epoch 41/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0643 - prc: 0.9439\n",
      "Epoch 42/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0627 - prc: 0.9454\n",
      "Epoch 43/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0634 - prc: 0.9438\n",
      "Epoch 44/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0658 - prc: 0.9377\n",
      "Epoch 45/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0663 - prc: 0.9326\n",
      "Epoch 46/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0657 - prc: 0.9385\n",
      "Epoch 47/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0607 - prc: 0.9490\n",
      "Epoch 48/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0601 - prc: 0.9477\n",
      "Epoch 49/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0582 - prc: 0.9539\n",
      "Epoch 50/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0569 - prc: 0.9562\n",
      "Epoch 51/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0572 - prc: 0.9524\n",
      "Epoch 52/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0754 - prc: 0.9288\n",
      "Epoch 53/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0635 - prc: 0.9431\n",
      "Epoch 54/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0624 - prc: 0.9426\n",
      "Epoch 55/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0597 - prc: 0.9492\n",
      "Epoch 56/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0617 - prc: 0.9402\n",
      "Epoch 57/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0597 - prc: 0.9484\n",
      "Epoch 58/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0624 - prc: 0.9427\n",
      "Epoch 59/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0521 - prc: 0.9649\n",
      "Epoch 60/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0544 - prc: 0.9603\n",
      "Epoch 61/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0552 - prc: 0.9598\n",
      "Epoch 62/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0523 - prc: 0.9633\n",
      "Epoch 63/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0603 - prc: 0.9476\n",
      "Epoch 64/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0594 - prc: 0.9486\n",
      "Epoch 65/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0597 - prc: 0.9462\n",
      "Epoch 66/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0543 - prc: 0.9594\n",
      "Epoch 67/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0559 - prc: 0.9551\n",
      "Epoch 68/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0588 - prc: 0.9490\n",
      "Epoch 69/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0512 - prc: 0.9645\n",
      "Epoch 70/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0559 - prc: 0.9537\n",
      "Epoch 71/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0612 - prc: 0.9446\n",
      "Epoch 72/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0576 - prc: 0.9519\n",
      "Epoch 73/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0675 - prc: 0.9335\n",
      "Epoch 74/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0634 - prc: 0.9348\n",
      "Epoch 75/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0506 - prc: 0.9636\n",
      "36/36 [==============================] - 0s 2ms/step\n",
      "Epoch 1/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0933 - prc: 0.8833\n",
      "Epoch 2/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0838 - prc: 0.8990\n",
      "Epoch 3/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0733 - prc: 0.9211\n",
      "Epoch 4/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0819 - prc: 0.8997\n",
      "Epoch 5/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0735 - prc: 0.9228\n",
      "Epoch 6/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0767 - prc: 0.9144\n",
      "Epoch 7/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0701 - prc: 0.9236\n",
      "Epoch 8/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0673 - prc: 0.9344\n",
      "Epoch 9/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0830 - prc: 0.9176\n",
      "Epoch 10/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0720 - prc: 0.9241\n",
      "Epoch 11/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0661 - prc: 0.9331\n",
      "Epoch 12/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0562 - prc: 0.9567\n",
      "Epoch 13/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0609 - prc: 0.9455\n",
      "Epoch 14/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0631 - prc: 0.9417\n",
      "Epoch 15/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0643 - prc: 0.9384\n",
      "Epoch 16/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0651 - prc: 0.9360\n",
      "Epoch 17/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0632 - prc: 0.9381\n",
      "Epoch 18/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0581 - prc: 0.9484\n",
      "Epoch 19/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0668 - prc: 0.9321\n",
      "Epoch 20/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0646 - prc: 0.9378\n",
      "Epoch 21/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0616 - prc: 0.9431\n",
      "Epoch 22/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0548 - prc: 0.9577\n",
      "Epoch 23/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0586 - prc: 0.9479\n",
      "Epoch 24/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0561 - prc: 0.9546\n",
      "Epoch 25/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0567 - prc: 0.9534\n",
      "Epoch 26/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0630 - prc: 0.9384\n",
      "Epoch 27/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0571 - prc: 0.9483\n",
      "Epoch 28/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0519 - prc: 0.9595\n",
      "Epoch 29/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0505 - prc: 0.9634\n",
      "Epoch 30/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0580 - prc: 0.9516\n",
      "Epoch 31/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0557 - prc: 0.9517\n",
      "Epoch 32/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0618 - prc: 0.9427\n",
      "Epoch 33/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0579 - prc: 0.9488\n",
      "Epoch 34/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0553 - prc: 0.9547\n",
      "Epoch 35/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0493 - prc: 0.9647\n",
      "Epoch 36/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0498 - prc: 0.9645\n",
      "Epoch 37/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0561 - prc: 0.9577\n",
      "Epoch 38/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0547 - prc: 0.9533\n",
      "Epoch 39/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0517 - prc: 0.9617\n",
      "Epoch 40/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0477 - prc: 0.9688\n",
      "Epoch 41/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0501 - prc: 0.9637\n",
      "Epoch 42/75\n",
      "72/72 [==============================] - 1s 14ms/step - loss: 0.0508 - prc: 0.9618\n",
      "Epoch 43/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0497 - prc: 0.9645\n",
      "Epoch 44/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0522 - prc: 0.9593\n",
      "Epoch 45/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0471 - prc: 0.9685\n",
      "Epoch 46/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0515 - prc: 0.9619\n",
      "Epoch 47/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0508 - prc: 0.9613\n",
      "Epoch 48/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0509 - prc: 0.9607\n",
      "Epoch 49/75\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.0567 - prc: 0.9497\n",
      "Epoch 50/75\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.0492 - prc: 0.9643\n",
      "Epoch 51/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0477 - prc: 0.9662\n",
      "Epoch 52/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0518 - prc: 0.9602\n",
      "Epoch 53/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0509 - prc: 0.9620\n",
      "Epoch 54/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0524 - prc: 0.9595\n",
      "Epoch 55/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0540 - prc: 0.9515\n",
      "Epoch 56/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0580 - prc: 0.9482\n",
      "Epoch 57/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0634 - prc: 0.9395\n",
      "Epoch 58/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0581 - prc: 0.9482\n",
      "Epoch 59/75\n",
      "72/72 [==============================] - 1s 13ms/step - loss: 0.0506 - prc: 0.9621\n",
      "Epoch 60/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0435 - prc: 0.9726\n",
      "Epoch 61/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0470 - prc: 0.9674\n",
      "Epoch 62/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0451 - prc: 0.9719\n",
      "Epoch 63/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0430 - prc: 0.9740\n",
      "Epoch 64/75\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.0433 - prc: 0.9727\n",
      "Epoch 65/75\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.0477 - prc: 0.9659\n",
      "Epoch 66/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0472 - prc: 0.9644\n",
      "Epoch 67/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0462 - prc: 0.9677\n",
      "Epoch 68/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0460 - prc: 0.9679\n",
      "Epoch 69/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0470 - prc: 0.9674\n",
      "Epoch 70/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0648 - prc: 0.9495\n",
      "Epoch 71/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0645 - prc: 0.9417\n",
      "Epoch 72/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0468 - prc: 0.9687\n",
      "Epoch 73/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0411 - prc: 0.9758\n",
      "Epoch 74/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0477 - prc: 0.9662\n",
      "Epoch 75/75\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.0428 - prc: 0.9734\n",
      "36/36 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "acc_score = []\n",
    "re_score = []\n",
    "pre_score = []\n",
    "f_score = []\n",
    "auroc = []\n",
    "auprc = []\n",
    "\n",
    "resample = NeighbourhoodCleaningRule() \n",
    "\n",
    "X = FeatureSet\n",
    "y = y_values\n",
    "\n",
    "k = 5\n",
    "ep = 75\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "X_test =  scaler.transform(X) \n",
    "Train_shape = X.shape\n",
    "\n",
    "X = X.reshape(int(Train_shape[0]/numberOfWindows),numberOfWindows,numberOfFeatures)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "# enumerate the splits and summarize the distributions\n",
    "\n",
    "for train_ix, val_ix in skf.split(X,y):\n",
    "    # select rows\n",
    "    X_train, X_val = X[train_ix], X[val_ix]\n",
    "    y_train, y_val = y[train_ix], y[val_ix]\n",
    "    \n",
    "    resample.fit_resample(X_train[:,:,0], y_train)\n",
    "    X_train = X_train[resample.sample_indices_]\n",
    "    y_train = y_train[resample.sample_indices_]\n",
    "    \n",
    "    history4 = model.fit(X_train,y_train, epochs = ep,batch_size=64)\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred = list(map(lambda x: 0 if x<0.5 else 1, y_pred))\n",
    "    #class_labels = np.argmax(y_pred, axis=1)\n",
    "    y_actu = pd.Series(y_val)\n",
    "    y_pred = pd.Series(y_pred)\n",
    "     \n",
    "    sensitivity = recall_score(y_actu, y_pred,pos_label = 1, average='binary')\n",
    "    precision = precision_score(y_actu, y_pred,pos_label = 1, average='binary')\n",
    "    f1_value = f1_score(y_actu, y_pred,pos_label = 1, average='binary')\n",
    "    prc = average_precision_score(y_actu, y_pred,pos_label = 1)\n",
    "    roc = roc_auc_score(y_actu, y_pred)\n",
    "    accuracy = accuracy_score(y_actu, y_pred)\n",
    "    \n",
    "    acc_score.append(accuracy)\n",
    "    re_score.append(sensitivity)\n",
    "    pre_score.append(precision)\n",
    "    f_score.append(f1_value)\n",
    "    auroc.append(roc)\n",
    "    auprc.append(prc)\n",
    "   \n",
    "avg_acc_score = sum(acc_score)/k\n",
    "avg_recall_score = sum(re_score)/k\n",
    "avg_precision_score = sum(pre_score)/k\n",
    "avg_f1_score = sum(f_score)/k\n",
    "avg_roc_score = sum(auroc)/k\n",
    "avg_prc_score = sum(auprc)/k\n",
    "\n",
    "\n",
    "sensitivity = avg_recall_score\n",
    "precision = avg_precision_score\n",
    "accuracy = avg_acc_score\n",
    "f1_score = avg_f1_score\n",
    "auroc = avg_roc_score\n",
    "auprc = avg_prc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of each fold - [0.9064685314685315, 0.9082167832167832, 0.923951048951049, 0.9396853146853147, 0.9492563429571304]\n",
      "Avg accuracy : 0.9255156042557615\n",
      "Recall of each fold - [0.24271844660194175, 0.4174757281553398, 0.4519230769230769, 0.625, 0.7184466019417476]\n",
      "Avg Reccall : 0.4911127707244212\n",
      "Precision of each fold - [0.46296296296296297, 0.48863636363636365, 0.6103896103896104, 0.6842105263157895, 0.7184466019417476]\n",
      "Avg Precision : 0.5929292130492948\n",
      "F1_Score of each fold - [0.3184713375796179, 0.4502617801047121, 0.5193370165745856, 0.6532663316582915, 0.7184466019417476]\n",
      "Avg F1_score : 0.531956613571791\n",
      "AUROC of each fold - 0.7298901435828563\n",
      "Avg AUROC : 0.7298901435828563\n",
      "AUPRC of each fold - 0.35318540112438473\n",
      "Avg AUPRC : 0.35318540112438473\n"
     ]
    }
   ],
   "source": [
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))\n",
    "print('Recall of each fold - {}'.format(re_score))\n",
    "print('Avg Reccall : {}'.format(avg_recall_score))\n",
    "print('Precision of each fold - {}'.format(pre_score))\n",
    "print('Avg Precision : {}'.format(avg_precision_score))\n",
    "print('F1_Score of each fold - {}'.format(f_score))\n",
    "print('Avg F1_score : {}'.format(avg_f1_score))\n",
    "print('AUROC of each fold - {}'.format(auroc))\n",
    "print('Avg AUROC : {}'.format(avg_roc_score))\n",
    "print('AUPRC of each fold - {}'.format(auprc))\n",
    "print('Avg AUPRC : {}'.format(avg_prc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "\n",
    "csv_columns = ['model-type','precision','sensitivity','f1-score','accuracy','AUROC','AUPRC','NumberOfWindows','Epochs']\n",
    "dict_data = [{'model-type':'ANN', 'precision': precision,'sensitivity': sensitivity,'f1-score': f1_score,'accuracy': accuracy,'NumberOfWindows':numberOfWindows,\"Epochs\":ep,'AUROC':auroc,'AUPRC':auprc}]\n",
    "metric_file = \"Results/Results_ANN.csv\"\n",
    "\n",
    "file_exists = os.path.isfile(metric_file)\n",
    "try:\n",
    "    with open(metric_file, 'a') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        for data in dict_data:\n",
    "            writer.writerow(data)\n",
    "except IOError:\n",
    "    print(\"I/O error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48feb6360a00eb0f47f9c89bd4d436ba37b01dbe1c2db3c2c6dfcc0146625bee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
